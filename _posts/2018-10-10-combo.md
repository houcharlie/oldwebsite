---
layout: post
title: Notes on Near-Optimal Time and Sample Complexities for DMDP with a Generative Model
permalink: /blog/:title
---

These some notes I've compiled over the course of reading [this paper](https://arxiv.org/pdf/1806.01492.pdf), which was published recently by Sidford, Wang et al.  I came across this paper at the suggestion of one of the authors in the paper, who is a [professor](http://www.princeton.edu/~mengdiw/) in my department.  I want crystalize my thoughts to test and record my understanding, but maybe this will be useful to others trying to get started in RL research.

This post will contain the ideas behind the paper.  If you are not familiar with reinforcement learning, please check this [link](https://houcharlie.github.io/blog/mdp) for a blog post describing the main ideas behind the definitions and problem formulations.

The main purpose of this paper is to design an algorithm that is asymptotically the best for finding the best policy in a discounted Markov Decision Process setup.

The most straightforward way of computing an optimal policy is like this:

$$
\begin{equation}
v^{i}(s) \leftarrow max_{a} r(s,a) + P_{s,a}^{T}v^{i-1}
\end{equation}
$$

$$
\begin{equation}
\pi^{i}(s) \leftarrow argmax_{a} r(s,a) + P_{s,a}^{T}v^{i}
\end{equation}
$$

This is the standard way to solve an markov decision process.  For the first update rule, what we are doing is iteratively trying to find the fixed point of $$T(v)$$.  The optimal value $$v$$ is the one that finds the fixed point.  The fixed point gives you the true value of $$v$$ if you were to follow the optimal policy, e.g. the expected payoff of starting from any state if you follow the optimal policy.

For the second update rule, the best policy is greedily chosen to be the best action that we can take in light of what we currently believe to be the expected payoffs.

If we had complete access to $$P$$, then this would be really easy.  However, the problem that we are interested in is one where we do not know $$P$$ and have to sample to approximate it.

In light of this, there are results that show that convergence of $$v^{i}$$ is based on how well we can approximate $$v^{i}$$, e.g. if we can approximate well, then we can converge faster with the approximated values.  But that does not necessarily mean that our $$\pi$$ is going to get close quickly.

To get around this, they use the *monotonicity technique* in their algorithm to force the convergence of $$\pi^{i}$$ to be at least equal to the convergence of $$\v^{i}$$, which allows the algorithm to converge faster in terms of time and samples (fewer iterations means fewer calculates and fewer samples as each iteration takes samples).

Second, they use the *variance reduction technique* to reduce the number of samples that are needed to calculate a quality estimate for $$v^{i}$, allowing the algorithm to converge faster in terms of samples. 

Third, they show that these techniques used together give a better bound than what was originally thought, by using the Bernstein inequality (a type of concentration bound).  


### The monotonicity technique
The idea is that we make sure that 
$$
\begin{equation}
v^{i} \leq T_{\pi^{i}}(v^{i})
\end{equation}
$$
is true with high probability (to find out how we ensure this is true, go to the Bernstein technique section).  In words, it means that with newer policies, our expected payoffs get better, which is something we would like to remain true!  Now assume that after $$R$$ iterations, the value that we have is $$\epsilon$$-optimal.  Then we have
$$
\begin{equation}
v^{R} \leq T_{\pi^{R}}(v^{R}) \leq T^{\infty}_{\pi^{R}}(v^{R}) = v^{\pi^{R}} \leq v^{*}
\end{equation}
$$
Which means that $$v^{\pi^{R}}$$ is also $$\epsilon$$-optimal, therefore $$\pi^{R}$$ is also $$\epsilon$$-optimal (by definition).  In words, this means that the policy $$\pi^{R}$$ will produce a fixed point that gives expected payoffs close to $$v^{*}$. 

### The variance reduction technique
The idea here is to write
$$
\begin{equation}
v^{i}(s) \leftarrow max_{a} [r(s,a) + P_{s,a}^{T}(v^{i-1} - v^{0}) + P_{s,a}^{T}v^{0}]
\end{equation}
$$
First, $P_{s,a}^{T}v^{0}$ only needs to be estimated once at the beginning of the algorithm, so its cost is low.  Second, because $$\|v^{i-1} - v^{0} \|$$ is small due to monotonicity ($$v^{i}$$ will never be farther away from $$v^{0}$$ than $$v^{0}$$ is from $$v^{*}$$), the variance of the estimate of it is small.  Altogether, this means that the number of samples that we need to estimate $$v^{i}$$ is low.

### The Bernstein technique
They use concentration inequalities here to show that the accumulated error from their algorithm is smaller than what one might think superficially.  They do this by first bounding the accumulated error from playing a policy by doing something similar to Cauchy-Schwartz.  Inside their upper bound, there is a value that is exactly equal to the variance of the value given by playing the policy.  Because the total return is bounded, this is used to bound the accumulated error.  

This isn't just useful for bounding the error, however.  They use this error bound in order to enforce the monotonicity.  They use the error bound, and subtract it from the values of $g^{i}$ (which are the values they use to approximate $P_{s,a}^{T}(v^{i-1} - v^{0})$) so that the estimates are always on their lower end, preventing non-monotonicity.  The danger we want to avoid is an estimate from the previous iteration being on the higher end and the next being on the lower end due to variance and messing up the monotonicity.  So this assures that the monotonicity properity is kept with high probability.


### Why are we only running the algorithm to halve the error, and then re-running?
Now, one thing that they do that might seem strange is that they run the algorithm until the error is one half what it is originally.  Then they re-run it.  Wouldn't it be easier just to run the algorithm and wait for the algorithm to just converge?  Well, the reality is that the results they show are all dependent on the initial bound on the error $$\|v^{0} - v^{*}\|_{\infty}$$.  I swept this under the rug a little bit, but you can see hints of it in some of what I said earlier.  Things like  "because $$\|v^{i-1} - v^{0} \|$$ is small due to monotonicity ($$v^{i}$$ will never be farther away from $$v^{0}$$ than $$v^{0}$$ is from $$v^{*}$$)" are dependent on the original bound on the error.  

The way that we fix this dependency on the initial bound is by re-starting the algorithm.  This way, the results get successively better as the bound gets smaller.  The number of samples we have to take and the time needed to reach completion (as they all depend on the initial bound) will improve.


### Conclusion
In this paper, Sidwell, Wang et al. find a clever way to combine techniques together to forge an algorithm that performs the best asymptotically across all algorithms that work on DMDPs.  This algorithm achieves the lower bound up to log factors, so perhaps improving on it in terms of runtime isn't too useful anymore.  What would be interesting to see is how well this algorithm would perform in real-world situations compared to baselines found in industry.  I have a suspicion that in its current state it is probably infeasible for most applications, so it would be interesting to figure out what kind of tweaks or approximations might make this usable but also retain its original character.

I learned a lot from reading through this paper, and I hope others might learn a little bit from the ideas I wrote in this post!


